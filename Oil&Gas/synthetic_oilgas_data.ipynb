{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fd3ab6-36bc-492e-82de-8fd68b2071f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory created: synthetic_oil_gas_data\n",
      "Production End Date set to 2023-12-31 for faster testing of sensor readings.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Global Configurations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "import os\n",
    "from tqdm.notebook import tqdm # For progress bar in Jupyter\n",
    "\n",
    "# Initialize Faker for realistic data\n",
    "fake = Faker()\n",
    "Faker.seed(42) # for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Configuration Parameters ---\n",
    "NUM_FIELDS = 10\n",
    "NUM_WELLS_PER_FIELD_RANGE = (10, 20) # Each field will have between 5 and 30 wells\n",
    "NUM_RIGS = 15\n",
    "PRODUCTION_START_DATE = datetime(2015, 1, 1)\n",
    "# Important: Temporarily reduce end date for faster sensor_readings generation during testing\n",
    "PRODUCTION_END_DATE = datetime(2023, 12, 31) # Original\n",
    "#PRODUCTION_END_DATE = datetime(2016, 12, 31) # Reduced for faster testing of sensor_readings\n",
    "\n",
    "SENSOR_READINGS_PER_WELL_PER_DAY = 12 # Hourly readings (24 / 2 = 12)\n",
    "FAILURE_RATE_PER_WELL_PER_YEAR = 0.5 # Average failures per well per year\n",
    "MAINTENANCE_FREQUENCY_DAYS = 90 # Routine maintenance every 90 days\n",
    "TRANSPORT_QUANTITY_THRESHOLD_BBL = 5000 # Shipments trigger when well accumulates this much oil\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = 'synthetic_oil_gas_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory created: {OUTPUT_DIR}\")\n",
    "print(f\"Production End Date set to {PRODUCTION_END_DATE.strftime('%Y-%m-%d')} for faster testing of sensor readings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c85a9c-cfc1-490e-bcc3-108cd4895733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def generate_date_series(start_date, end_date):\n",
    "    \"\"\"Generates a list of dates between start_date and end_date.\"\"\"\n",
    "    delta = end_date - start_date\n",
    "    return [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "\n",
    "def generate_timestamp_series(start_datetime, end_datetime, interval_hours=1):\n",
    "    \"\"\"Generates a list of timestamps at specified intervals.\"\"\"\n",
    "    current = start_datetime\n",
    "    timestamps = []\n",
    "    while current <= end_datetime:\n",
    "        timestamps.append(current)\n",
    "        current += timedelta(hours=interval_hours)\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8450e01-6e48-43cc-be76-8742b8da4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fields table...\n",
      "Generated fields.csv with 10 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: generate_fields Function and Execution\n",
    "\n",
    "def generate_fields(num_fields):\n",
    "    print(\"Generating fields table...\")\n",
    "    fields_data = []\n",
    "    countries = ['USA', 'Canada', 'Mexico', 'Brazil', 'Saudi Arabia', 'Norway', 'UK', 'Nigeria', 'Russia', 'Australia', 'Oman']\n",
    "    regions = ['Permian Basin', 'Alberta Basin', 'North Sea', 'Gulf of Mexico', 'Arabian Desert', 'Siberia', 'Niger Delta', 'Offshore Brazil', 'Western Australia']\n",
    "\n",
    "    for i in range(num_fields):\n",
    "        field_id = f'FLD-{i+1:03d}'\n",
    "        name = fake.city_prefix() + ' ' + fake.word().capitalize() + ' Field'\n",
    "        country = random.choice(countries)\n",
    "        region = random.choice(regions)\n",
    "        discovery_date = fake.date_between(start_date='-50y', end_date='-10y')\n",
    "        operator = fake.company()\n",
    "        total_reserves_est = round(random.uniform(100, 5000) * 1_000_000, 0) # Million barrels\n",
    "        development_stage = random.choice(['Exploration', 'Development', 'Production', 'Mature', 'Abandonment'])\n",
    "\n",
    "        fields_data.append({\n",
    "            'field_id': field_id,\n",
    "            'name': name,\n",
    "            'country': country,\n",
    "            'region': region,\n",
    "            'discovery_date': discovery_date.strftime('%Y-%m-%d'),\n",
    "            'operator': operator,\n",
    "            'total_reserves_est_bbl': total_reserves_est,\n",
    "            'development_stage': development_stage\n",
    "        })\n",
    "    df_fields = pd.DataFrame(fields_data)\n",
    "    df_fields.to_csv(os.path.join(OUTPUT_DIR, 'fields.csv'), index=False)\n",
    "    print(f\"Generated fields.csv with {len(df_fields)} records.\")\n",
    "    return df_fields\n",
    "\n",
    "# Execute field generation\n",
    "df_fields = generate_fields(NUM_FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209846a4-25f8-4734-870f-c2bde82b829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating wells table...\n",
      "Generated wells.csv with 168 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: generate_wells Function and Execution\n",
    "\n",
    "def generate_wells(df_fields):\n",
    "    print(\"Generating wells table...\")\n",
    "    wells_data = []\n",
    "    well_types = ['Vertical', 'Horizontal', 'Directional']\n",
    "    well_statuses = ['Producing', 'Shut-in', 'Abandoned', 'Drilling', 'Plugged & Abandoned']\n",
    "\n",
    "    well_counter = 0\n",
    "    for _, field in df_fields.iterrows():\n",
    "        num_wells_in_field = random.randint(*NUM_WELLS_PER_FIELD_RANGE)\n",
    "        for i in range(num_wells_in_field):\n",
    "            well_counter += 1\n",
    "            well_id = f'WELL-{well_counter:05d}'\n",
    "            \n",
    "            # Corrected: Convert field['discovery_date'] string to datetime.date object\n",
    "            field_discovery_date_obj = datetime.strptime(field['discovery_date'], '%Y-%m-%d').date()\n",
    "            production_end_date_obj = PRODUCTION_END_DATE.date()\n",
    "\n",
    "            spud_date = fake.date_between(start_date=field_discovery_date_obj, end_date=production_end_date_obj)\n",
    "            \n",
    "            completion_date = spud_date + timedelta(days=random.randint(30, 365))\n",
    "            \n",
    "            # Ensure completion date doesn't go beyond end of production data period\n",
    "            if completion_date > PRODUCTION_END_DATE.date():\n",
    "                 completion_date = PRODUCTION_END_DATE.date()\n",
    "\n",
    "            depth_m = round(random.uniform(1000, 4000), 2)\n",
    "            \n",
    "            # Location relative to field (e.g., slightly offset from a central point)\n",
    "            lat = round(random.uniform(20.0, 30.0) + random.uniform(-0.5, 0.5), 6) # Base lat/lon for region\n",
    "            lon = round(random.uniform(50.0, 60.0) + random.uniform(-0.5, 0.5), 6)\n",
    "\n",
    "            status = random.choice(well_statuses)\n",
    "            \n",
    "            # Simulate anomalies\n",
    "            if random.random() < 0.02: # 2% abandoned wells\n",
    "                status = 'Abandoned'\n",
    "            if random.random() < 0.01: # 1% missing depth\n",
    "                depth_m = np.nan\n",
    "            if random.random() < 0.005: # 0.5% location errors\n",
    "                lat = round(lat + random.uniform(-5.0, 5.0), 6)\n",
    "                lon = round(lon + random.uniform(-5.0, 5.0), 6)\n",
    "\n",
    "            wells_data.append({\n",
    "                'well_id': well_id,\n",
    "                'field_id': field['field_id'],\n",
    "                'well_type': random.choice(well_types),\n",
    "                'spud_date': spud_date.strftime('%Y-%m-%d'),\n",
    "                'completion_date': completion_date.strftime('%Y-%m-%d'),\n",
    "                'status': status,\n",
    "                'depth_m': depth_m,\n",
    "                'location_lat': lat,\n",
    "                'location_lon': lon\n",
    "            })\n",
    "    df_wells = pd.DataFrame(wells_data)\n",
    "    df_wells.to_csv(os.path.join(OUTPUT_DIR, 'wells.csv'), index=False)\n",
    "    print(f\"Generated wells.csv with {len(df_wells)} records.\")\n",
    "    return df_wells\n",
    "\n",
    "# Execute wells generation\n",
    "df_wells = generate_wells(df_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5624bcc-3da9-4e2a-a670-3b9891aedae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rigs table...\n",
      "Generated rigs.csv with 15 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: generate_rigs Function and Execution\n",
    "\n",
    "def generate_rigs(num_rigs):\n",
    "    print(\"Generating rigs table...\")\n",
    "    rigs_data = []\n",
    "    rig_types = ['Land Rig', 'Offshore Platform', 'Jack-up Rig', 'Drillship', 'Semi-submersible']\n",
    "    rig_contractors = [fake.company() + ' Drilling', fake.company() + ' Services']\n",
    "    rig_statuses = ['Active', 'Idle', 'Maintenance', 'Stacked']\n",
    "\n",
    "    for i in range(num_rigs):\n",
    "        rig_id = f'RIG-{i+1:03d}'\n",
    "        rig_type = random.choice(rig_types)\n",
    "        contractor = random.choice(rig_contractors)\n",
    "        \n",
    "        # Capacity based on type\n",
    "        if 'Land' in rig_type:\n",
    "            capacity = random.randint(1000, 3000) # HP\n",
    "        elif 'Offshore' in rig_type:\n",
    "            capacity = random.randint(5000, 15000) # ft drilling depth\n",
    "        else: # Generic capacity\n",
    "            capacity = random.randint(2000, 10000)\n",
    "\n",
    "        status = random.choice(rig_statuses)\n",
    "        last_inspection_date = fake.date_between(start_date='-5y', end_date='today')\n",
    "\n",
    "        rigs_data.append({\n",
    "            'rig_id': rig_id,\n",
    "            'type': rig_type,\n",
    "            'contractor': contractor,\n",
    "            'capacity_unit': 'HP' if 'Land' in rig_type else 'ft' if 'Offshore' in rig_type else 'misc_unit',\n",
    "            'capacity': capacity,\n",
    "            'status': status,\n",
    "            'last_inspection_date': last_inspection_date.strftime('%Y-%m-%d')\n",
    "        })\n",
    "    df_rigs = pd.DataFrame(rigs_data)\n",
    "    df_rigs.to_csv(os.path.join(OUTPUT_DIR, 'rigs.csv'), index=False)\n",
    "    print(f\"Generated rigs.csv with {len(df_rigs)} records.\")\n",
    "    return df_rigs\n",
    "\n",
    "# Execute rigs generation\n",
    "df_rigs = generate_rigs(NUM_RIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7e14a2-f225-40fa-83ff-51be65c39b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating drilling_events table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8b7ba6835b4bc8a752803caa6a6176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drilling Events for Wells: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated drilling_events.csv with 1512 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: generate_drilling_events Function and Execution\n",
    "\n",
    "def generate_drilling_events(df_wells, df_rigs):\n",
    "    print(\"Generating drilling_events table...\")\n",
    "    drilling_events_data = []\n",
    "    event_types = ['Spud', 'Rig Up', 'Drill Tophole', 'Run Casing', 'Cement Casing', \n",
    "                   'Drill Intermediate', 'Logging', 'Drill Production', 'Run Tubing', \n",
    "                   'Perforate', 'Stimulate', 'Rig Down', 'Move Rig']\n",
    "\n",
    "    active_rigs = df_rigs[df_rigs['status'] == 'Active']['rig_id'].tolist()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for _, well in tqdm(df_wells[df_wells['status'].isin(['Drilling', 'Producing', 'Shut-in', 'Plugged & Abandoned'])].iterrows(), \n",
    "                        desc=\"Drilling Events for Wells\"):\n",
    "        well_spud_date = datetime.strptime(well['spud_date'], '%Y-%m-%d')\n",
    "        well_completion_date = datetime.strptime(well['completion_date'], '%Y-%m-%d') if pd.notna(well['completion_date']) else None\n",
    "\n",
    "        current_event_date = well_spud_date\n",
    "        \n",
    "        # Assign a rig for the drilling phase\n",
    "        if not active_rigs:\n",
    "            continue # Skip if no active rigs to assign\n",
    "        assigned_rig = random.choice(active_rigs)\n",
    "\n",
    "        # Sequence of events\n",
    "        events_sequence = ['Spud', 'Rig Up', 'Drill Tophole', 'Run Casing', 'Cement Casing', \n",
    "                           'Drill Intermediate', 'Logging', 'Drill Production', 'Rig Down']\n",
    "        \n",
    "        if well_completion_date: # Only if well was completed\n",
    "             events_sequence.extend(['Run Tubing', 'Perforate', 'Stimulate'])\n",
    "\n",
    "        for event_type in events_sequence:\n",
    "            start_date = current_event_date\n",
    "            duration_days = random.randint(2, 30)\n",
    "            \n",
    "            # Simulate delays or overlaps\n",
    "            if random.random() < 0.05: # 5% chance of delay\n",
    "                duration_days += random.randint(5, 15)\n",
    "            if random.random() < 0.01: # 1% chance of overlapping event\n",
    "                start_date -= timedelta(days=random.randint(1, 5)) \n",
    "\n",
    "            end_date = start_date + timedelta(days=duration_days)\n",
    "\n",
    "            # Ensure event doesn't go beyond completion date if it's a completion-related event\n",
    "            if well_completion_date and end_date.date() > well_completion_date.date() and event_type in ['Run Tubing', 'Perforate', 'Stimulate']:\n",
    "                end_date = well_completion_date\n",
    "                start_date = end_date - timedelta(days=random.randint(1, 10)) # Adjust start date to fit\n",
    "\n",
    "            # Anomaly: Missing rig assignment\n",
    "            rig_for_event = assigned_rig\n",
    "            if random.random() < 0.005: # 0.5% missing rig assignments\n",
    "                rig_for_event = np.nan\n",
    "\n",
    "            drilling_events_data.append({\n",
    "                'well_id': well['well_id'],\n",
    "                'rig_id': rig_for_event,\n",
    "                'event_type': event_type,\n",
    "                'start_date': start_date.strftime('%Y-%m-%d'),\n",
    "                'end_date': end_date.strftime('%Y-%m-%d')\n",
    "            })\n",
    "            current_event_date = end_date + timedelta(days=random.randint(1, 5)) # Gap between events\n",
    "    \n",
    "    df_drilling_events = pd.DataFrame(drilling_events_data)\n",
    "    df_drilling_events.to_csv(os.path.join(OUTPUT_DIR, 'drilling_events.csv'), index=False)\n",
    "    print(f\"Generated drilling_events.csv with {len(df_drilling_events)} records.\")\n",
    "    return df_drilling_events\n",
    "\n",
    "# Execute drilling events generation\n",
    "df_drilling_events = generate_drilling_events(df_wells, df_rigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff09cab6-4995-4898-8026-8a6618a98f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating production_logs table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ffe59b8d804e7abaf5a8d27269b489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production Logs for Wells: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated production_logs.csv with 82253 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: generate_production_logs Function and Execution\n",
    "\n",
    "def generate_production_logs(df_wells):\n",
    "    print(\"Generating production_logs table...\")\n",
    "    production_logs_data = []\n",
    "    \n",
    "    # Filter for producing wells that have a completion date\n",
    "    producing_wells = df_wells[\n",
    "        (df_wells['status'] == 'Producing') & \n",
    "        (df_wells['completion_date'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    # Pre-calculate initial rates and decline for each well\n",
    "    for index, well in producing_wells.iterrows():\n",
    "        producing_wells.loc[index, 'initial_oil_rate_bopd'] = random.uniform(50, 2000)\n",
    "        producing_wells.loc[index, 'decline_rate_exp'] = random.uniform(0.0001, 0.0015) # Daily exponential decline\n",
    "        producing_wells.loc[index, 'initial_gas_oil_ratio_scf_bbl'] = random.uniform(500, 3000)\n",
    "        producing_wells.loc[index, 'initial_water_cut'] = random.uniform(0.05, 0.4) # Initial water cut\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for _, well in tqdm(producing_wells.iterrows(), desc=\"Production Logs for Wells\"):\n",
    "        well_completion_date = datetime.strptime(well['completion_date'], '%Y-%m-%d')\n",
    "        \n",
    "        # Adjust start date for production if completion is after the global start date\n",
    "        prod_start = max(well_completion_date, PRODUCTION_START_DATE)\n",
    "\n",
    "        current_date = prod_start\n",
    "        while current_date <= PRODUCTION_END_DATE:\n",
    "            days_since_completion = (current_date - well_completion_date).days\n",
    "            \n",
    "            # Exponential decline model for oil\n",
    "            oil_rate = well['initial_oil_rate_bopd'] * np.exp(-well['decline_rate_exp'] * days_since_completion)\n",
    "            \n",
    "            # Add some daily noise\n",
    "            noise = np.random.normal(0, oil_rate * 0.05) # 5% noise relative to rate\n",
    "            oil_rate = max(0, oil_rate + noise)\n",
    "\n",
    "            # Simulate sudden shut-ins or drops\n",
    "            if random.random() < 0.005: # 0.5% chance of sudden drop/shut-in\n",
    "                oil_rate *= random.uniform(0.1, 0.5) if random.random() < 0.5 else 0 # 50% chance of partial drop, 50% for full shut-in\n",
    "\n",
    "            # Gas and water production based on oil and dynamic water cut/GOR (simplified)\n",
    "            gas_rate = oil_rate * well['initial_gas_oil_ratio_scf_bbl'] / 1000 # convert to mcf\n",
    "            water_cut_increase = (days_since_completion / (365 * 10)) * random.uniform(0.01, 0.05) # Water cut increases over time\n",
    "            current_water_cut = min(0.95, well['initial_water_cut'] + water_cut_increase)\n",
    "            water_rate = (oil_rate / (1 - current_water_cut) * current_water_cut) if current_water_cut < 1 else oil_rate # Avoid division by zero\n",
    "            water_rate = max(0, water_rate)\n",
    "\n",
    "            choke_size = round(random.uniform(12, 64), 2) # e.g., in 64ths of an inch\n",
    "            tubing_pressure = round(random.uniform(1000, 5000), 2) # psi, varies with production\n",
    "\n",
    "            production_logs_data.append({\n",
    "                'well_id': well['well_id'],\n",
    "                'date': current_date.strftime('%Y-%m-%d'),\n",
    "                'oil_bbl': round(oil_rate, 2),\n",
    "                'gas_mcf': round(gas_rate, 2),\n",
    "                'water_bbl': round(water_rate, 2),\n",
    "                'choke_size': choke_size,\n",
    "                'tubing_pressure_psi': tubing_pressure\n",
    "            })\n",
    "            current_date += timedelta(days=1)\n",
    "            \n",
    "    df_production_logs = pd.DataFrame(production_logs_data)\n",
    "    df_production_logs.to_csv(os.path.join(OUTPUT_DIR, 'production_logs.csv'), index=False)\n",
    "    print(f\"Generated production_logs.csv with {len(df_production_logs)} records.\")\n",
    "    return df_production_logs\n",
    "\n",
    "# Execute production logs generation\n",
    "df_production_logs = generate_production_logs(df_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ea4032-d24e-44cb-81b3-57640d353893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sensor_readings table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7d85b3208744bbb00aa1a5ca9220aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sensor Readings per Well:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sensor_readings.csv with 14615136 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: generate_sensor_readings Function and Execution (with progress bar)\n",
    "\n",
    "def generate_sensor_readings(df_wells):\n",
    "    print(\"Generating sensor_readings table...\")\n",
    "    sensor_readings_data = []\n",
    "    \n",
    "    # Consider only wells that are producing or shut-in for sensor readings\n",
    "    active_wells = df_wells[df_wells['status'].isin(['Producing', 'Shut-in'])].copy()\n",
    "    \n",
    "    # Use tqdm to show progress for wells\n",
    "    for _, well in tqdm(active_wells.iterrows(), desc=\"Generating Sensor Readings per Well\", total=len(active_wells)):\n",
    "        # Assign a few sensors per well\n",
    "        num_sensors_per_well = random.randint(2, 5)\n",
    "        well_sensor_ids = [f'{well[\"well_id\"]}-S{j+1:02d}' for j in range(num_sensors_per_well)]\n",
    "\n",
    "        start_time = datetime.strptime(well['spud_date'], '%Y-%m-%d').replace(hour=0, minute=0, second=0)\n",
    "        end_time = PRODUCTION_END_DATE.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "        timestamps = generate_timestamp_series(start_time, end_time, interval_hours=24 / SENSOR_READINGS_PER_WELL_PER_DAY)\n",
    "\n",
    "        for sensor_id in well_sensor_ids:\n",
    "            # Base values for sensor readings\n",
    "            base_pressure = random.uniform(500, 3000)\n",
    "            base_temperature = random.uniform(30, 100)\n",
    "            base_vibration = random.uniform(0.1, 1.0)\n",
    "\n",
    "            # Introduce a potential \"fault\"\n",
    "            fault_start_time = None\n",
    "            if random.random() < 0.1: # 10% chance of a fault\n",
    "                # Ensure fault occurs within the timestamp range\n",
    "                if len(timestamps) > 1:\n",
    "                    fault_start_time = random.choice(timestamps[len(timestamps)//4 : len(timestamps)*3//4]) # Fault occurs in middle 50% of period\n",
    "                else:\n",
    "                    fault_start_time = timestamps[0] # If only one timestamp, use it\n",
    "                fault_duration_hours = random.randint(24*7, 24*30) # Lasts for 1-4 weeks\n",
    "\n",
    "            for i, ts in enumerate(timestamps):\n",
    "                # Simulate sine wave with noise for cyclical behavior\n",
    "                # Ensure time_factor doesn't cause issues if start_time == ts\n",
    "                time_diff_seconds = (ts - start_time).total_seconds()\n",
    "                time_factor = time_diff_seconds / (365.25 * 24 * 3600) if time_diff_seconds > 0 else 0\n",
    "                \n",
    "                pressure = base_pressure + 100 * np.sin(time_factor * 2 * np.pi) + np.random.normal(0, 10)\n",
    "                temperature = base_temperature + 5 * np.sin(time_factor * 2 * np.pi * 12) + np.random.normal(0, 1) # Daily cycle\n",
    "                vibration = base_vibration + 0.1 * np.sin(time_factor * 2 * np.pi * 24) + np.random.normal(0, 0.05) # More frequent oscillation\n",
    "\n",
    "                # Simulate faults/drift\n",
    "                if fault_start_time and ts >= fault_start_time and ts <= fault_start_time + timedelta(hours=fault_duration_hours):\n",
    "                    if 'pressure' in sensor_id: # Simulate pressure drop\n",
    "                        pressure *= random.uniform(0.5, 0.8) # 20-50% drop\n",
    "                    elif 'temperature' in sensor_id: # Simulate temperature spike\n",
    "                        temperature += random.uniform(10, 30)\n",
    "                    elif 'vibration' in sensor_id: # Simulate vibration spike\n",
    "                        vibration += random.uniform(0.5, 2.0)\n",
    "                \n",
    "                # Anomaly: NaNs and outliers\n",
    "                if random.random() < 0.001: # 0.1% chance of NaN\n",
    "                    pressure = np.nan\n",
    "                elif random.random() < 0.0005: # 0.05% chance of outlier\n",
    "                    pressure *= random.uniform(0.1, 3.0) # Extreme value\n",
    "\n",
    "                sensor_readings_data.append({\n",
    "                    'sensor_id': sensor_id,\n",
    "                    'well_id': well['well_id'],\n",
    "                    'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'pressure_psi': round(pressure, 2),\n",
    "                    'temperature_c': round(temperature, 2),\n",
    "                    'vibration_g': round(vibration, 4)\n",
    "                })\n",
    "    df_sensor_readings = pd.DataFrame(sensor_readings_data)\n",
    "    df_sensor_readings.to_csv(os.path.join(OUTPUT_DIR, 'sensor_readings.csv'), index=False)\n",
    "    print(f\"Generated sensor_readings.csv with {len(df_sensor_readings)} records.\")\n",
    "    return df_sensor_readings\n",
    "\n",
    "# Execute sensor readings generation\n",
    "df_sensor_readings = generate_sensor_readings(df_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef13411-1be4-49f9-87ee-1bb86c0125ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating equipment_failures table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb5e83b829549ae8fc7c923936ea09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Equipment Failures for Wells: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated equipment_failures.csv with 465 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: generate_equipment_failures Function and Execution\n",
    "\n",
    "def generate_equipment_failures(df_wells):\n",
    "    print(\"Generating equipment_failures table...\")\n",
    "    equipment_failures_data = []\n",
    "    failure_types = ['Pump Failure', 'Valve Leak', 'Pipeline Blockage', 'Electrical Fault', \n",
    "                     'Sensor Malfunction', 'Compressor Issue', 'Power Outage', 'Corrosion']\n",
    "    \n",
    "    # Consider only active wells for failures\n",
    "    active_wells = df_wells[df_wells['status'].isin(['Producing', 'Shut-in'])].copy()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for _, well in tqdm(active_wells.iterrows(), desc=\"Generating Equipment Failures for Wells\"):\n",
    "        well_start_date = datetime.strptime(well['completion_date'], '%Y-%m-%d') if pd.notna(well['completion_date']) else datetime.strptime(well['spud_date'], '%Y-%m-%d')\n",
    "        well_end_date = PRODUCTION_END_DATE\n",
    "\n",
    "        # Determine number of failures based on annual rate\n",
    "        days_active = (well_end_date - well_start_date).days\n",
    "        expected_failures = (days_active / 365) * FAILURE_RATE_PER_WELL_PER_YEAR\n",
    "        num_failures = np.random.poisson(expected_failures) # Poisson distribution for counts of events\n",
    "\n",
    "        for _ in range(num_failures):\n",
    "            failure_date = well_start_date + timedelta(days=random.randint(0, days_active))\n",
    "            if failure_date > well_end_date: # Prevent failures after data end\n",
    "                continue\n",
    "\n",
    "            failure_type = random.choice(failure_types)\n",
    "            downtime_hrs = round(random.uniform(4, 24*7), 2) # 4 hours to 7 days\n",
    "            equipment_id = f'EQ-{fake.bothify(text='??###')}' # Example equipment ID format\n",
    "            \n",
    "            # Optional: Add severity, repair cost, response time\n",
    "            severity = random.choice(['Minor', 'Moderate', 'Major'])\n",
    "            repair_cost_usd = round(random.uniform(500, 50000), 2)\n",
    "            response_time_hrs = round(random.uniform(0.5, 48), 2)\n",
    "\n",
    "            equipment_failures_data.append({\n",
    "                'equipment_id': equipment_id,\n",
    "                'well_id': well['well_id'],\n",
    "                'failure_type': failure_type,\n",
    "                'failure_date': failure_date.strftime('%Y-%m-%d'),\n",
    "                'downtime_hrs': downtime_hrs,\n",
    "                'severity': severity,\n",
    "                'repair_cost_usd': repair_cost_usd,\n",
    "                'response_time_hrs': response_time_hrs\n",
    "            })\n",
    "    df_equipment_failures = pd.DataFrame(equipment_failures_data)\n",
    "    df_equipment_failures.to_csv(os.path.join(OUTPUT_DIR, 'equipment_failures.csv'), index=False)\n",
    "    print(f\"Generated equipment_failures.csv with {len(df_equipment_failures)} records.\")\n",
    "    return df_equipment_failures\n",
    "\n",
    "# Execute equipment failures generation\n",
    "df_equipment_failures = generate_equipment_failures(df_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6a72e8-e086-4dc7-90e2-e5fec2d2b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prices table...\n",
      "Generated prices.csv with 3287 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: generate_prices Function and Execution\n",
    "\n",
    "def generate_prices():\n",
    "    print(\"Generating prices table...\")\n",
    "    prices_data = []\n",
    "    \n",
    "    current_date = PRODUCTION_START_DATE\n",
    "    # Base prices and volatility\n",
    "    base_oil_price = 60 # USD/bbl\n",
    "    base_gas_price = 3.0 # USD/mcf\n",
    "    base_water_disposal_cost = 0.5 # USD/bbl\n",
    "\n",
    "    while current_date <= PRODUCTION_END_DATE:\n",
    "        day_of_year = current_date.timetuple().tm_yday\n",
    "        year_fraction = (current_date - PRODUCTION_START_DATE).days / 365.25\n",
    "\n",
    "        # Simulate trend + seasonality + noise for oil price\n",
    "        oil_price = base_oil_price + (year_fraction * 5) + (10 * np.sin(day_of_year * 2 * np.pi / 365)) + np.random.normal(0, 2)\n",
    "        oil_price = max(30, oil_price) # Floor price\n",
    "\n",
    "        # Simulate trend + noise for gas price\n",
    "        gas_price = base_gas_price + (year_fraction * 0.1) + (0.5 * np.sin(day_of_year * 2 * np.pi / 365)) + np.random.normal(0, 0.1)\n",
    "        gas_price = max(1.5, gas_price) # Floor price\n",
    "\n",
    "        # Water disposal cost (relatively stable with minor fluctuations)\n",
    "        water_disposal_cost = base_water_disposal_cost + np.random.normal(0, 0.05)\n",
    "        water_disposal_cost = max(0.1, water_disposal_cost)\n",
    "\n",
    "        prices_data.append({\n",
    "            'date': current_date.strftime('%Y-%m-%d'),\n",
    "            'oil_price_usd_bbl': round(oil_price, 2),\n",
    "            'gas_price_usd_mcf': round(gas_price, 2),\n",
    "            'water_disposal_cost_usd_bbl': round(water_disposal_cost, 2)\n",
    "        })\n",
    "        current_date += timedelta(days=1) # Moved inside loop for correction\n",
    "    \n",
    "    df_prices = pd.DataFrame(prices_data)\n",
    "    df_prices.to_csv(os.path.join(OUTPUT_DIR, 'prices.csv'), index=False)\n",
    "    print(f\"Generated prices.csv with {len(df_prices)} records.\")\n",
    "    return df_prices\n",
    "\n",
    "# Execute prices generation\n",
    "df_prices = generate_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdf16fe-182a-425d-ba19-f631d67136e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating transport_logs table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272cfd2a73204c5095a1d3cfa4e3c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Transport Logs:   0%|          | 0/82253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated transport_logs.csv with 3699 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: generate_transport_logs Function and Execution\n",
    "\n",
    "def generate_transport_logs(df_production_logs):\n",
    "    print(\"Generating transport_logs table...\")\n",
    "    transport_logs_data = []\n",
    "    transporters = [fake.company() + ' Logistics', fake.company() + ' Transport']\n",
    "    destinations = ['Refinery A', 'Pipeline Hub B', 'Storage C']\n",
    "    shipment_statuses = ['Scheduled', 'In Transit', 'Delivered', 'Delayed', 'Cancelled']\n",
    "    \n",
    "    # Aggregate daily production to simulate shipments\n",
    "    # Ensure 'date' column is datetime type before sorting and cumsum\n",
    "    df_production_logs['date'] = pd.to_datetime(df_production_logs['date'])\n",
    "    \n",
    "    # Calculate cumulative oil production per well\n",
    "    df_production_logs_sorted = df_production_logs.sort_values(by=['well_id', 'date'])\n",
    "    df_production_logs_sorted['cumulative_oil'] = df_production_logs_sorted.groupby('well_id')['oil_bbl'].cumsum()\n",
    "\n",
    "    shipment_counter = 0\n",
    "    # Track the last shipment quantity for each well\n",
    "    last_shipped_oil = {well_id: 0 for well_id in df_production_logs_sorted['well_id'].unique()}\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for _, row in tqdm(df_production_logs_sorted.iterrows(), desc=\"Generating Transport Logs\", total=len(df_production_logs_sorted)):\n",
    "        well_id = row['well_id']\n",
    "        current_cumulative_oil = row['cumulative_oil']\n",
    "        \n",
    "        # Check if enough oil has accumulated for a shipment\n",
    "        if current_cumulative_oil - last_shipped_oil[well_id] >= TRANSPORT_QUANTITY_THRESHOLD_BBL:\n",
    "            shipment_counter += 1\n",
    "            shipment_id = f'SHIP-{shipment_counter:06d}'\n",
    "            \n",
    "            quantity_bbl = round(current_cumulative_oil - last_shipped_oil[well_id], 2)\n",
    "            shipment_date = row['date']\n",
    "            destination = random.choice(destinations)\n",
    "            transporter = random.choice(transporters)\n",
    "            status = random.choice(shipment_statuses)\n",
    "\n",
    "            # Simulate anomalies\n",
    "            if random.random() < 0.005: # 0.5% missed shipments (don't record this one)\n",
    "                pass\n",
    "            else:\n",
    "                if random.random() < 0.01: # 1% wrong destinations\n",
    "                    destination = fake.city() + ' Port'\n",
    "                if random.random() < 0.02: # 2% delays\n",
    "                    status = 'Delayed'\n",
    "                    shipment_date += timedelta(days=random.randint(1, 10))\n",
    "\n",
    "                transport_logs_data.append({\n",
    "                    'shipment_id': shipment_id,\n",
    "                    'well_id': well_id,\n",
    "                    'date': shipment_date.strftime('%Y-%m-%d'),\n",
    "                    'quantity_bbl': quantity_bbl,\n",
    "                    'destination': destination,\n",
    "                    'transporter': transporter,\n",
    "                    'status': status\n",
    "                })\n",
    "                last_shipped_oil[well_id] = current_cumulative_oil # Update last shipped\n",
    "                \n",
    "    df_transport_logs = pd.DataFrame(transport_logs_data)\n",
    "    df_transport_logs.to_csv(os.path.join(OUTPUT_DIR, 'transport_logs.csv'), index=False)\n",
    "    print(f\"Generated transport_logs.csv with {len(df_transport_logs)} records.\")\n",
    "    return df_transport_logs\n",
    "\n",
    "# Execute transport logs generation\n",
    "df_transport_logs = generate_transport_logs(df_production_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e674950-8b90-40e3-b0e9-38143aa26abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating maintenance_logs table...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8f5c40c2774cce8e6fd5542f9e3995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Maintenance Logs for Wells: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated maintenance_logs.csv with 4154 records.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: generate_maintenance_logs Function and Execution\n",
    "\n",
    "def generate_maintenance_logs(df_wells, df_equipment_failures):\n",
    "    print(\"Generating maintenance_logs table...\")\n",
    "    maintenance_logs_data = []\n",
    "    activities_routine = ['Routine Inspection', 'Preventive Maintenance', 'Calibration', 'Lubrication', 'Filter Replacement']\n",
    "    activities_reactive = ['Repair', 'Troubleshooting', 'Component Replacement', 'System Restart']\n",
    "    technicians = [fake.name() for _ in range(20)] # Pool of technician names\n",
    "\n",
    "    # Convert failure_date to datetime for comparison\n",
    "    df_equipment_failures['failure_date'] = pd.to_datetime(df_equipment_failures['failure_date'])\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for _, well in tqdm(df_wells[df_wells['status'].isin(['Producing', 'Shut-in'])].iterrows(), desc=\"Generating Maintenance Logs for Wells\"):\n",
    "        well_completion_date_str = well['completion_date']\n",
    "        if pd.isna(well_completion_date_str):\n",
    "            continue # Skip wells without a completion date (or use spud date as fallback)\n",
    "        \n",
    "        well_completion_date = datetime.strptime(well_completion_date_str, '%Y-%m-%d')\n",
    "        \n",
    "        # Routine maintenance\n",
    "        current_date_routine = well_completion_date\n",
    "        while current_date_routine <= PRODUCTION_END_DATE:\n",
    "            if random.random() < 0.95: # 95% chance to log routine maintenance on schedule\n",
    "                maintenance_logs_data.append({\n",
    "                    'well_id': well['well_id'],\n",
    "                    'event_date': current_date_routine.strftime('%Y-%m-%d'),\n",
    "                    'activity': random.choice(activities_routine),\n",
    "                    'technician': random.choice(technicians),\n",
    "                    'duration_hrs': round(random.uniform(2, 8), 2),\n",
    "                    'cost_usd': round(random.uniform(100, 1000), 2)\n",
    "                })\n",
    "            else: # Anomaly: skipped schedule\n",
    "                pass \n",
    "            current_date_routine += timedelta(days=MAINTENANCE_FREQUENCY_DAYS)\n",
    "\n",
    "        # Reactive maintenance based on equipment failures\n",
    "        well_failures = df_equipment_failures[df_equipment_failures['well_id'] == well['well_id']]\n",
    "        for _, failure in well_failures.iterrows():\n",
    "            maintenance_date = failure['failure_date'] + timedelta(hours=random.uniform(0, failure['response_time_hrs']))\n",
    "            # Ensure maintenance date is within valid range\n",
    "            if maintenance_date.date() > PRODUCTION_END_DATE.date():\n",
    "                continue\n",
    "\n",
    "            maintenance_logs_data.append({\n",
    "                'well_id': well['well_id'],\n",
    "                'event_date': maintenance_date.strftime('%Y-%m-%d'),\n",
    "                'activity': f\"Reactive: {random.choice(activities_reactive)} ({failure['failure_type']})\",\n",
    "                'technician': random.choice(technicians),\n",
    "                'duration_hrs': round(failure['downtime_hrs'] * random.uniform(0.8, 1.2), 2), # Maintenance duration related to downtime\n",
    "                'cost_usd': round(failure['repair_cost_usd'] * random.uniform(0.9, 1.1), 2)\n",
    "            })\n",
    "            \n",
    "            # Anomaly: Duplicate logs (2% chance)\n",
    "            if random.random() < 0.02:\n",
    "                maintenance_logs_data.append({\n",
    "                    'well_id': well['well_id'],\n",
    "                    'event_date': (maintenance_date + timedelta(days=1)).strftime('%Y-%m-%d'), # Duplicate on next day\n",
    "                    'activity': f\"Reactive: {random.choice(activities_reactive)} (Duplicate)\",\n",
    "                    'technician': random.choice(technicians),\n",
    "                    'duration_hrs': round(random.uniform(1, 3), 2),\n",
    "                    'cost_usd': round(random.uniform(50, 200), 2)\n",
    "                })\n",
    "    \n",
    "    df_maintenance_logs = pd.DataFrame(maintenance_logs_data)\n",
    "    df_maintenance_logs.to_csv(os.path.join(OUTPUT_DIR, 'maintenance_logs.csv'), index=False)\n",
    "    print(f\"Generated maintenance_logs.csv with {len(df_maintenance_logs)} records.\")\n",
    "    return df_maintenance_logs\n",
    "\n",
    "# Execute maintenance logs generation\n",
    "df_maintenance_logs = generate_maintenance_logs(df_wells, df_equipment_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e5afd2-d4ac-448a-b2e7-5d927cf09dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All synthetic data tables generated successfully! ---\n",
      "Check the 'synthetic_oil_gas_data' directory for CSV files.\n",
      "Remember to reset PRODUCTION_END_DATE to its original value if you reduced it for testing.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Final Confirmation\n",
    "\n",
    "print(\"\\n--- All synthetic data tables generated successfully! ---\")\n",
    "print(f\"Check the '{OUTPUT_DIR}' directory for CSV files.\")\n",
    "print(\"Remember to reset PRODUCTION_END_DATE to its original value if you reduced it for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f9e8f-9690-4d70-a965-cd6716b761cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
